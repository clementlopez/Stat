\documentclass{article}
    \usepackage[utf8]{inputenc}
    
    \title{APP1 Stats - Ensimag}
    \author{}
    \date{17 Octobre 2018}
    
    \usepackage{amsfonts}
    \usepackage{subcaption}
    \usepackage{graphicx}
    \usepackage[a4paper, margin=1in]{geometry}
    \usepackage{listings}
    \usepackage{minted}
    \usepackage{amsmath}
    \usepackage{amsfonts}
    \usepackage{amssymb}\usepackage{amsmath}
    \usepackage{color}
    \usepackage{indentfirst}
    
    \renewcommand{\contentsname}{Sommaire}
    \renewcommand{\thesection}{\Roman{section}}
    
\begin{document}
        
\maketitle

Voir le fichier \textit{app1.r} pour les commentaires associés.

\section{1ère Méthode (Marius) : Méthode du moment d'ordre 1 (espérance)}

\subsection{Preuve que l'estimateur de Marius n'est pas biaisé (espérance)}

Loi uniforme :\\

Si $X_1...X_n$ sont indépendants et de même loi $U\tilde (a,b)$, alors l'estimateur de $\mathbb{E}(X)=\frac{a+b}{2}$.

\begin{equation}
    \begin{aligned}
        \mathbb{E}(\Theta_{Marius})&=\frac{2}{n}\sum_{i=1}^n \mathbb{E}(X_i)\\
        &=\frac{2}{n}\sum_{i=1}^n \frac{a+b}{2}\\
        &=\frac{2}{n}n\frac{a+b}{2}\\
        &=b
    \end{aligned}
\end{equation}

En moyenne, notre estimateur est égal à $\theta$, donc non biaisé.

\subsection{Preuve que l'estimateur de Marius est convergent (variance)}

\begin{equation}
    \begin{aligned}
        Var(\Theta_{Marius})&=Var(\frac{2}{n}\sum_{i=1}^n X_i)\\
        &=(\frac{2}{n})^2Var(\sum_{i=1}^n X_i)\\
        &=(\frac{2}{n})^2\sum_{i=1}^n Var(X_i)\\
        &=(\frac{2}{n})^2\sum_{i=1}^n \frac{(b-a)^2}{12}\\
        &=(\frac{2}{n})^2n\frac{(b-a)^2}{12}\\
        &=\frac{4}{n^2}n\frac{b^2}{12}=\frac{4nb^2}{n^2 12}\\
        &=\frac{4b^2}{n 12}=\frac{b^2}{3n}\\
        &\simeq \frac{1}{n}
    \end{aligned}
\end{equation}

Donc l'estimateur est dit convergent. C'est un bon estimateur.

\section{2ème Méthode (Jeannette) : Méthode du maximum de vraisemblance}

On veut maximiser :

\begin{equation}
    \begin{aligned}
        f_{(X_1...X_n)}&=\prod_{i=1}^{n}f_{Xi}\\
        &=\prod_{i=1}^{n}\frac{1}{b}=\frac{1}{b^n}
    \end{aligned}
\end{equation}

On veut donc trouver le $\frac{1}{b^n}$ le plus grand possible. On souhaite alors trouver le $b$ le plus petit possible, i.e. le maximum obtenu dans le jeu de données sans quoi on risque d'exclure des valeurs. Ceci correspond donc bien à la méthode d'estimation utilisée par Jeannette.

$$
\Leftrightarrow max_{i=1}^n \{X_i\} = \Theta_{Jeannette}
$$

On calcule la fonction de répartition :

\begin{equation}
    \begin{aligned}
        F_X(x)&=\mathbb{P}(max_{i=1}^nX_i\leq x)\\
        &=\mathbb{P}(\bigcap_{i=1}^n\{X_i\leq x\})\\
        &=\prod_{i=1}^n\mathbb{P}(X_i\leq x)\\
        &=\prod_{i=1}^n \frac{x-a}{b-a}\\
        &=\prod_{i=1}^n \frac{x}{b}=\frac{x^n}{b^n}\\
        f_X(x)&=F_X'(x)=n\frac{x^{n-1}}{b^n}\\
        \text{Donc, on a : }\Theta_{Jeannette}=\mathbb{E}(X)&=\int_0^b x(n\frac{x^{n-1}}{b^n})dx\\
        &=\int_0^b n\frac{x^n}{b^n}dx\\
        &=\frac{n}{b^n}\int_0^b x^ndx\\
        &=\frac{n}{b^n}[\frac{x^{n+1}}{n+1}]_0^b\\
        &=\frac{n}{b^n}\frac{b^{n+1}}{n+1}\\
        &=\frac{n}{n+1}b
    \end{aligned}
\end{equation}

Le biais de notre estimateur est de $\frac{n}{n+1}$.

\end{document}